from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableLambda
from typing import Optional, Annotated, TypedDict, Generator
import json
import requests
import time
import sys
import os
import re
import logging

# ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° import
from utils.config import config_manager
from utils.embedding import (
    EmbeddingModel, 
    load_documents_from_directory, 
    create_document_chunks,
    create_document_chunks_parallel
)
from utils.search import create_faiss_vectorstore, load_faiss_vectorstore, get_retriever, similarity_search
from utils.prompt_templates import (
    SUMMARY_PROMPT_TEMPLATE,
    REPORT_PROMPT_TEMPLATE,
    FALLBACK_PROMPT_TEMPLATE
)
from utils.output_format import format_answer, remove_duplicates
from utils.llm import LLMClient
from utils.parallel import ProgressTracker, parallel_process

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ==========================
# âœ… JSON ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€
# ==========================
def clean_json_text(text: str) -> str:
    """JSON íŒŒì‹± ì „ ì œì–´ ë¬¸ì ì œê±°"""
    # ì œì–´ ë¬¸ì ì œê±° (ì¤„ë°”ê¿ˆ, íƒ­, ìºë¦¬ì§€ ë¦¬í„´ ë“±)
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ==========================
# âœ… ì„¤ì • ê¸°ë°˜ ìƒìˆ˜
# ==========================
def get_config():
    """ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return config_manager

# ê¸°ë³¸ ì„¤ì • (ì„¤ì • íŒŒì¼ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
DEFAULT_LLM_CONFIG = {
    "model": "mistral",
    "temperature": 0.1,
    "num_predict": 3000,
    "top_k": 3,
    "top_p": 0.9,
    "timeout": 900
}

# ==========================
# âœ… ì¸ë±ìŠ¤ ìë™ ìƒì„±/ë¡œë“œ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ ê°•í™”)
# ==========================
def get_or_build_embedding_index(use_parallel: bool = True, max_workers: int = 4):
    """
    ì„ë² ë”© ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ ë° ì§„í–‰ë¥  í‘œì‹œ ê°•í™”)
    
    Args:
        use_parallel: ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
        max_workers: ìµœëŒ€ ì›Œì»¤ ìˆ˜
    """
    try:
        paths_config = config_manager.get_paths_config()
        docs_dir = paths_config.get("docs_dir", "./docs/embedding/")
        index_path = paths_config.get("index_path", "./faiss_index")
        
        faiss_path = f"{index_path}.faiss"
        pkl_path = f"{index_path}.pkl"
        
        embedding_config = config_manager.get_embedding_config()
        embedding_model = EmbeddingModel()
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        if os.path.exists(faiss_path) and os.path.exists(pkl_path):
            logger.info(f"ê¸°ì¡´ ì„ë² ë”© ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {index_path}")
            try:
                vectorstore = load_faiss_vectorstore(index_path, embedding_model)
                retriever = get_retriever(vectorstore)
                logger.info("ì„ë² ë”© ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
                return retriever
            except Exception as e:
                logger.error(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.info("ì†ìƒëœ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
        logger.info(f"ì„ë² ë”© ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {docs_dir}")
        if not os.path.exists(docs_dir):
            logger.error(f"ë¬¸ì„œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {docs_dir}")
            return None
        
        # ë¬¸ì„œ ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
        documents = load_documents_from_directory(
            docs_dir, 
            use_parallel=use_parallel, 
            max_workers=max_workers
        )
        if not documents:
            logger.error(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_dir}")
            return None
        
        # ì²­í¬ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
        all_chunks, all_metadata = create_document_chunks_parallel(
            documents,
            use_parallel=use_parallel,
            max_workers=max_workers
        )
        
        if not all_chunks:
            logger.error("ë¬¸ì„œ ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ë¬¸ì„œ ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±/ì €ì¥
        logger.info(f"ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {len(all_chunks)}ê°œ ì²­í¬")
        vectorstore = create_faiss_vectorstore(all_chunks, all_metadata, embedding_model, index_path)
        retriever = get_retriever(vectorstore)
        logger.info(f"ì„ë² ë”© ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ: {index_path}")
        return retriever
        
    except Exception as e:
        logger.error(f"ì„ë² ë”© ì¸ë±ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ==========================
# âœ… 1. ë³´ê³ ì„œ & ìš”ì•½ ìŠ¤í‚¤ë§ˆ ì •ì˜
# ==========================
class TechReport(BaseModel):
    title: str = Field(description="ë³´ê³ ì„œ ì œëª©")
    abstract: str = Field(description="ìš”ì•½")
    introduction: str = Field(description="ì„œë¡ ")
    background: str = Field(description="ë°°ê²½")
    main_content: str = Field(description="ë³¸ë¬¸ ë‚´ìš©")
    conclusion: str = Field(description="ê²°ë¡ ")

class Summary(BaseModel):
    summary: str = Field(description="ê°„ëµ ìš”ì•½")

# ==========================
# âœ… 2. LangGraph ìƒíƒœ ì •ì˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
# ==========================
class ReportState(TypedDict):
    # ì…ë ¥ ë°ì´í„°
    source_text: str
    question: str
    top_k: int
    
    # í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ
    prompt: str
    response: str
    parsed: dict
    
    # ìƒíƒœ ê´€ë¦¬
    error: str
    retry_count: int
    mode: str
    
    # ê²€ìƒ‰ ê´€ë ¨
    search_results: list
    search_success: bool
    
    # LLM ì„¤ì • (ì„ íƒì )
    llm_config: Optional[dict]
    
    # ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
    streaming_enabled: bool

# ==========================
# âœ… 3. ì„ë² ë”© ê²€ìƒ‰ í•¨ìˆ˜
# ==========================
def highlight_keywords(text: str, keywords: list) -> str:
    # CLIì—ì„œ í‚¤ì›Œë“œë¥¼ ë…¸ë€ìƒ‰(ANSI)ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸
    def ansi_highlight(match):
        return f"\033[1;33m{match.group(0)}\033[0m"
    for kw in sorted(set(keywords), key=len, reverse=True):
        if kw.strip():
            # ë‹¨ì–´ ê²½ê³„ ê¸°ì¤€ìœ¼ë¡œë§Œ í•˜ì´ë¼ì´íŠ¸
            text = re.sub(rf'(?i)\b{re.escape(kw)}\b', ansi_highlight, text)
    return text

def extract_keywords(question: str) -> list:
    # ê°„ë‹¨í•˜ê²Œ ë„ì–´ì“°ê¸° ê¸°ì¤€ ë¶„ë¦¬ + ê¸¸ì´ 2 ì´ìƒë§Œ (ì‹¬í™”: konlpy ë“± í˜•íƒœì†Œ ë¶„ì„ ê°€ëŠ¥)
    words = re.findall(r'\w+', question)
    return [w for w in words if len(w) > 1]

def search_relevant_documents(question: str, retriever, k: Optional[int] = None, max_chars: Optional[int] = None):
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë°˜í™˜ (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)"""
    try:
        search_config = config_manager.get_search_config()
        k = int(k) if k is not None else int(search_config.get("default_top_k", 5))
        max_chars = int(max_chars) if max_chars is not None else int(config_manager.get("embedding.max_chars", 15000))
        
        if retriever is None:
            logger.error("ì„ë² ë”© ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ì„ë² ë”© ì¸ë±ìŠ¤ ì—†ìŒ)", []
        
        results = similarity_search(question, retriever, k=k)
        if not results:
            logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n(ë¬¸ì„œê°€ ì—†ê±°ë‚˜, ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.)", []
        
        relevant_docs = []
        total_chars = 0
        sources = []
        keywords = extract_keywords(question)
        max_doc_chars = search_config.get("max_doc_chars", 3500)
        
        for rank, doc in enumerate(results[:k], 1):
            similarity = "-"
            content = doc.page_content
            if len(content) > max_doc_chars:
                content = content[:max_doc_chars] + f"\n[ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì˜ë ¸ìŠµë‹ˆë‹¤... (ìœ ì‚¬ë„: {similarity})]"
            
            doc_highlighted = highlight_keywords(content, keywords)
            doc_text = f"[{rank}] ìœ ì‚¬ë„: {similarity}\n{doc_highlighted}"
            
            if total_chars + len(doc_text) > max_chars:
                remaining_chars = max_chars - total_chars
                if remaining_chars > 200:
                    doc_text = doc_text[:remaining_chars] + f"\n[ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì˜ë ¸ìŠµë‹ˆë‹¤... (ìœ ì‚¬ë„: {similarity})]"
                    relevant_docs.append(doc_text)
                    sources.append(doc.metadata.get('filename', doc.metadata.get('filepath', 'unknown')))
                break
            
            relevant_docs.append(doc_text)
            sources.append(doc.metadata.get('filename', doc.metadata.get('filepath', 'unknown')))
            total_chars += len(doc_text)
        
        result = "\n\n".join(relevant_docs)
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_docs)}ê°œ ë¬¸ì„œ, {len(result)}ì")
        return result, sources
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì—ëŸ¬: {e})", []
    if retriever is None:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ì„ë² ë”© ì¸ë±ìŠ¤ ì—†ìŒ)", []
    try:
        results = similarity_search(question, retriever, k=k)
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n(ë¬¸ì„œê°€ ì—†ê±°ë‚˜, ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.)", []
        # ë””ë²„ê·¸ ì •ë³´ëŠ” ì£¼ì„ ì²˜ë¦¬ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
        # print("[DEBUG] ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜:", len(results))
        # for i, doc in enumerate(results, 1):
        #     content = doc.page_content
        #     print(f"[DEBUG] ë¬¸ì„œ {i} ê¸¸ì´: {len(content)} | ì•ë¶€ë¶„: {content[:80]}")
        relevant_docs = []
        total_chars = 0
        sources = []
        keywords = extract_keywords(question)
        for rank, doc in enumerate(results[:5], 1):  # ìƒìœ„ 5ê°œ ì‚¬ìš©
            similarity = "-"
            max_doc_chars = 3500  # ê° ë¬¸ì„œ 3500ìê¹Œì§€
            content = doc.page_content
            if len(content) > max_doc_chars:
                content = content[:max_doc_chars] + f"\n[ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì˜ë ¸ìŠµë‹ˆë‹¤... (ìœ ì‚¬ë„: {similarity})]"
            doc_highlighted = highlight_keywords(content, keywords)
            doc_text = f"[{rank}] ìœ ì‚¬ë„: {similarity}\n{doc_highlighted}"
            if total_chars + len(doc_text) > max_chars:
                remaining_chars = max_chars - total_chars
                if remaining_chars > 200:
                    doc_text = doc_text[:remaining_chars] + f"\n[ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì˜ë ¸ìŠµë‹ˆë‹¤... (ìœ ì‚¬ë„: {similarity})]"
                    relevant_docs.append(doc_text)
                    sources.append(doc.metadata.get('filename', doc.metadata.get('filepath', 'unknown')))
                break
            relevant_docs.append(doc_text)
            sources.append(doc.metadata.get('filename', doc.metadata.get('filepath', 'unknown')))
            total_chars += len(doc_text)
        result = "\n\n".join(relevant_docs)
        # print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ í¬ê¸°: {len(result)}ì (ì œí•œ: {max_chars}ì)")
        # print(f"ğŸ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ: {len(relevant_docs)}ê°œ ë¬¸ì„œ")
        return result, sources
    except Exception as e:
        print(f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì—ëŸ¬: {e})", []

# ==========================
# âœ… 4. LangGraph ë…¸ë“œ ì •ì˜
# ==========================
def generate_prompt(state: ReportState) -> ReportState:
    new_state = {
        "source_text": state["source_text"],
        "question": state["question"],
        "top_k": state["top_k"],
        "prompt": "",
        "response": state.get("response", ""),
        "parsed": state.get("parsed", {}),
        "error": state.get("error", ""),
        "retry_count": state.get("retry_count", 0),
        "mode": state["mode"],
        "search_results": state.get("search_results", []),
        "search_success": state.get("search_success", True)
    }
    if "llm_config" in state and state["llm_config"] is not None:
        new_state["llm_config"] = state["llm_config"]
    # ìœ ì—°í•œ í”„ë¡¬í”„íŠ¸
    new_state["prompt"] = f"""
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ  ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸš¨ í•„ìˆ˜ ì§€ì‹œì‚¬í•­:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš” (JSON ì™¸ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€)
- ëª¨ë“  í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

ğŸ“ ì‘ì„± ê°€ì´ë“œ:
- ë¬¸ì„œì˜ í•µì‹¬ ì •ë³´ë¥¼ ìœ ì§€í•˜ë˜, ì°½ì˜ì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”
- ê° í•­ëª©ì€ ê³ ìœ í•œ ê´€ì ê³¼ ëª©ì ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
- êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
- ê° í•„ë“œëŠ” ìµœì†Œ ê¸¸ì´ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš” (abstract: 500ì, introduction: 500ì, background: 500ì, main_content: 800ì, conclusion: 500ì)
- ì˜ˆì‹œ, ì‚¬ë¡€, êµ¬ì²´ì  ìˆ˜ì¹˜, ì‹¤ë¬´ íŒ ë“±ì„ í¬í•¨í•˜ì—¬ í’ë¶€í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

âš ï¸ ì—„ê²©í•œ ê·œì¹™:
- JSON ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ ë¬¸ì„œì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- "ì¤‘ìš”í•©ë‹ˆë‹¤", "ë³µì¡í•©ë‹ˆë‹¤" ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì ì´ê³  ì°½ì˜ì ì¸ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ëª¨ë“  ë‚´ìš©ì€ ìì‹ ë§Œì˜ ì–¸ì–´ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”

ì…ë ¥ ë¬¸ì„œ:
========================
{state['source_text']}
========================
"""
    # print("[DEBUG] LLM í”„ë¡¬í”„íŠ¸ ì „ì²´:\n" + new_state["prompt"][:2000] + ("..." if len(new_state["prompt"]) > 2000 else ""))
    return ReportState(**new_state)

# call_modelì˜ num_predict ì œí•œ ì›ë³µ (DEFAULT_LLM_CONFIG ì‚¬ìš©) + ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
def call_model(state: ReportState) -> ReportState:
    from utils.llm import LLMClient
    new_state = {
        "source_text": state["source_text"],
        "question": state["question"],
        "top_k": state["top_k"],
        "prompt": state["prompt"],
        "response": "",
        "parsed": state.get("parsed", {}),
        "error": "",
        "retry_count": state.get("retry_count", 0),
        "mode": state["mode"],
        "search_results": state.get("search_results", []),
        "search_success": state.get("search_success", True),
        "streaming_enabled": state.get("streaming_enabled", False)
    }
    if "llm_config" in state and state["llm_config"] is not None:
        new_state["llm_config"] = state["llm_config"]
    llm_config = DEFAULT_LLM_CONFIG.copy()
    if "llm_config" in state and state["llm_config"] is not None:
        llm_config.update(state["llm_config"])
    timeout = max(llm_config["timeout"], 60)
    try:
        logger.info(f"LLM í˜¸ì¶œ ì‹œì‘: {llm_config['model']}")
        llm = LLMClient(model_name=llm_config["model"])
        
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™•ì¸
        if state.get("streaming_enabled", False):
            logger.info("ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ LLM í˜¸ì¶œ")
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìˆ˜ì§‘
            full_response = ""
            for chunk in llm.stream_generate(
                state["prompt"], 
                num_predict=llm_config["num_predict"], 
                temperature=llm_config["temperature"]
            ):
                print(chunk, end="", flush=True)
                full_response += chunk
            print()  # ì¤„ë°”ê¿ˆ
            new_state["response"] = full_response
        else:
            logger.info("ì¼ë°˜ ëª¨ë“œë¡œ LLM í˜¸ì¶œ")
            response = llm.generate(
                state["prompt"], 
                num_predict=llm_config["num_predict"], 
                temperature=llm_config["temperature"]
            )
            new_state["response"] = response
        
        new_state["error"] = ""
        logger.info("LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
    except Exception as e:
        error_msg = f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        new_state["error"] = error_msg
    return ReportState(**new_state)


def parse_response(state: ReportState) -> ReportState:
    from utils.llm import LLMClient
    new_state = {
        "source_text": state["source_text"],
        "question": state["question"],
        "top_k": state["top_k"],
        "prompt": state["prompt"],
        "response": state["response"],
        "parsed": {},
        "error": "",
        "retry_count": state.get("retry_count", 0),
        "mode": state["mode"],
        "search_results": state.get("search_results", []),
        "search_success": state.get("search_success", True)
    }
    if "llm_config" in state and state["llm_config"] is not None:
        new_state["llm_config"] = state["llm_config"]
    if state["error"]:
        new_state["error"] = state["error"]
        print(f"âš ï¸ ì´ì „ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {state['error']}")
        return ReportState(**new_state)
    if not state["response"]:
        new_state["error"] = "LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        print("âŒ LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return ReportState(**new_state)
    try:
        print("ğŸ”„ ì‘ë‹µ íŒŒì‹± ì¤‘...")
        # print(f"[DEBUG] ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(state['response'])}")
        # print(f"[DEBUG] ì›ë³¸ ì‘ë‹µ ì•ë¶€ë¶„: {state['response'][:500]}...")
        
        llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
        result = llm.parse_response(state["response"])
        
        # print(f"[DEBUG] íŒŒì‹± ê²°ê³¼ íƒ€ì…: {type(result)}")
        
        if isinstance(result, dict):
            # print(f"[DEBUG] íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬ í‚¤: {list(result.keys())}")
            # for k, v in result.items():
            #     print(f"[DEBUG] íŒŒì‹±ëœ í•­ëª©: {k} | ê¸¸ì´: {len(str(v))} | ê°’: {str(v)[:100]}")
            
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ì˜¤ë¥˜ê°€ ìˆì„ ë•Œë§Œ ì¶œë ¥)
            required_fields = ["title", "abstract", "introduction", "background", "main_content", "conclusion"]
            missing_fields = [field for field in required_fields if field not in result or not result[field]]
            if missing_fields:
                print(f"âš ï¸ ëˆ„ë½ëœ í•„ë“œ: {missing_fields}")
        # else:
        #     print(f"[DEBUG] íŒŒì‹± ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {result}")
        
        new_state["parsed"] = result
        new_state["error"] = ""
        print("âœ… ì‘ë‹µ íŒŒì‹± ì™„ë£Œ")
    except Exception as e:
        error_msg = f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        print(f"ğŸ“ ì›ë³¸ ì‘ë‹µ: {state['response'][:500]}...")
        new_state["parsed"] = {
            "raw_response": state["response"],
            "error": error_msg
        }
        new_state["error"] = error_msg
    return ReportState(**new_state)

def fix_prompt_and_retry(state: ReportState) -> ReportState:
    """í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ë° ì¬ì‹œë„ ë…¸ë“œ - source_textë¥¼ ë³€ê²½í•˜ì§€ ì•ŠìŒ"""
    # ìƒíƒœë¥¼ ë³µì‚¬í•˜ë˜ source_textëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    new_state = {
        "source_text": state["source_text"],  # ëª…ì‹œì ìœ¼ë¡œ ìœ ì§€
        "question": state["question"],
        "top_k": state["top_k"],
        "prompt": "",
        "response": state["response"],
        "parsed": state.get("parsed", {}),
        "error": state.get("error", ""),
        "retry_count": state.get("retry_count", 0) + 1,
        "mode": state["mode"],
        "search_results": state.get("search_results", []),
        "search_success": state.get("search_success", True)
    }
    
    # LLM ì„¤ì •ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ì¬ì‹œë„ ì‹œ í† í° ìˆ˜ ì¤„ì„)
    if "llm_config" in state and state["llm_config"] is not None:
        new_state["llm_config"] = state["llm_config"].copy()
        # ì¬ì‹œë„ ì‹œ í† í° ìˆ˜ë¥¼ ì¤„ì—¬ì„œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
        new_state["llm_config"]["num_predict"] = min(2000, state["llm_config"].get("num_predict", 4000))
    else:
        new_state["llm_config"] = DEFAULT_LLM_CONFIG.copy()
        new_state["llm_config"]["num_predict"] = 2000  # ì¬ì‹œë„ìš©ìœ¼ë¡œ í† í° ìˆ˜ ì¤„ì„
    
    print(f"ğŸ”„ ì¬ì‹œë„ {new_state['retry_count']}íšŒ - í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì¤‘...")
            # print(f"ğŸ“ ì¬ì‹œë„ ì„¤ì •: í† í° ìˆ˜ {new_state['llm_config']['num_predict']}")
    
    original_response = state["response"]
    
    if state["mode"] == "summary":
        new_state["prompt"] = f"""
ì´ì „ ì‘ë‹µì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{state['source_text'][:2000]}  # ë¬¸ì„œ ê¸¸ì´ ì œí•œ

ì§ˆë¬¸:
{state['question']}

ìš”êµ¬ì‚¬í•­:
- JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
- summary í•„ë“œì— ìš”ì•½ ë‚´ìš© í¬í•¨
- í•œêµ­ì–´ë¡œ ì‘ì„±
- 300ì ì´ìƒ

ì˜ˆì‹œ í˜•ì‹:
{{"summary": "ìš”ì•½ ë‚´ìš©"}}
"""
    else:
        new_state["prompt"] = f"""
ì´ì „ ì‘ë‹µì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸš¨ í•„ìˆ˜ ì§€ì‹œì‚¬í•­:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš” (JSON ì™¸ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€)
- ëª¨ë“  í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”

ë¬¸ì„œ:
{state['source_text'][:2000]}  # ë¬¸ì„œ ê¸¸ì´ ì œí•œ

ì§ˆë¬¸:
{state['question']}

ìš”êµ¬ì‚¬í•­:
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (JSON ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)
- ë‹¤ìŒ í•„ë“œë§Œ í¬í•¨: title, abstract, introduction, background, main_content, conclusion
- sections ë°°ì—´ì´ë‚˜ ë‹¤ë¥¸ í•„ë“œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
- ê° í•„ë“œëŠ” ìµœì†Œ ê¸¸ì´ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš” (abstract: 500ì, introduction: 500ì, background: 500ì, main_content: 800ì, conclusion: 500ì)
- ì˜ˆì‹œ, ì‚¬ë¡€, êµ¬ì²´ì  ìˆ˜ì¹˜, ì‹¤ë¬´ íŒ ë“±ì„ í¬í•¨í•˜ì—¬ í’ë¶€í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”

âš ï¸ ì—„ê²©í•œ ê·œì¹™:
- JSON ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ ë¬¸ì„œì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- "ì¤‘ìš”í•©ë‹ˆë‹¤", "ë³µì¡í•©ë‹ˆë‹¤" ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì ì´ê³  ì°½ì˜ì ì¸ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ëª¨ë“  ë‚´ìš©ì€ ìì‹ ë§Œì˜ ì–¸ì–´ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
"""
    
    # print("âœ… í”„ë¡¬í”„íŠ¸ ìˆ˜ì • ì™„ë£Œ (ë¬¸ì„œ ê¸¸ì´ ì œí•œ ì ìš©)")
    return ReportState(**new_state)

# ==========================
# âœ… 5. LangGraph êµ¬ì„± (ìˆ˜ì •)
# ==========================
builder = StateGraph(ReportState)
builder.add_node("generate_prompt", RunnableLambda(generate_prompt))
builder.add_node("call_model", RunnableLambda(call_model))
builder.add_node("parse_response", RunnableLambda(parse_response))
builder.add_node("fix_prompt_and_retry", RunnableLambda(fix_prompt_and_retry))

builder.set_entry_point("generate_prompt")
builder.add_edge("generate_prompt", "call_model")
builder.add_edge("call_model", "parse_response")
builder.add_conditional_edges(
    "parse_response",
    lambda state: END if not state["error"] or (state.get("retry_count") or 0) >= 1 else "fix_prompt_and_retry"
)
builder.add_edge("fix_prompt_and_retry", "call_model")

graph = builder.compile()

# ==========================
# âœ… 6. ìœ í‹¸ í•¨ìˆ˜
# ==========================
def determine_mode(question: str) -> str:
    summary_keywords = ["ìš”ì•½", "ê°„ë‹¨íˆ", "í•œì¤„", "ì§§ê²Œ"]
    report_keywords = ["ë³´ê³ ì„œ", "ì‘ì„±í•´ì¤˜", "ì •ë¦¬í•´ì¤˜", "êµ¬ì¡°í™”", "í•­ëª©ë³„", "ì „ë¬¸ì ìœ¼ë¡œ", "ë¶„ì„í•´ì¤˜", "ì„œìˆ í˜•"]

    if any(kw in question for kw in summary_keywords):
        return "summary"
    if any(kw in question for kw in report_keywords):
        return "report"
    return "free"

def regenerate_field(field: str, source_text: str, question: str, llm_config: Optional[dict] = None) -> str:
    """íŠ¹ì • í•„ë“œ ì¬ìƒì„±"""
    print(f"ğŸ” {field} í•­ëª© ê¸¸ì´ ë¶€ì¡± â†’ ìë™ ì¬ì‘ì„±")
    
    # LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    config = DEFAULT_LLM_CONFIG.copy()
    if llm_config:
        config.update(llm_config)
    
    # í† í° ìˆ˜ë¥¼ ì¤„ì—¬ì„œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
    config["num_predict"] = min(1500, config.get("num_predict", 2000))
    
    try:
        prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ {field} í•­ëª©ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{source_text[:3000]}

ì§ˆë¬¸:
{question}

ìš”êµ¬ì‚¬í•­:
- JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
- {field} í•„ë“œì— ë‚´ìš© í¬í•¨
- í•œêµ­ì–´ë¡œ ì‘ì„±
- 300ì ì´ìƒ
- ì œì–´ ë¬¸ì(ì¤„ë°”ê¿ˆ, íƒ­ ë“±) ì‚¬ìš© ê¸ˆì§€

ì˜ˆì‹œ í˜•ì‹:
{{"{field}": "ë‚´ìš©"}}
"""
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            headers={"Content-Type": "application/json"},
            data=json.dumps({
                "model": config["model"],
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": config["temperature"],
                    "num_predict": config["num_predict"],
                    "top_k": config["top_k"],
                    "top_p": config["top_p"]
                }
            }),
            timeout=config["timeout"]
        )
        
        if response.status_code == 200:
            resp_json = response.json()
            if "response" in resp_json:
                # JSON ì „ì²˜ë¦¬ ì ìš©
                cleaned_response = clean_json_text(resp_json["response"])
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    parsed = json.loads(cleaned_response)
                    if field in parsed:
                        return parsed[field]
                except json.JSONDecodeError as e:
                    print(f"í•„ë“œ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
                    return resp_json["response"]
        
        return f"{field} ì¬ìƒì„± ì‹¤íŒ¨"
        
    except Exception as e:
        print(f"í•„ë“œ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return f"{field} ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"

# ==========================
# âœ… 6. LLM ì„¤ì • êµ¬ì„± í•¨ìˆ˜
# ==========================
def configure_llm_settings(current_config: dict) -> dict:
    """LLM ì„¤ì •ì„ ëŒ€í™”í˜•ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” í•¨ìˆ˜"""
    print("\nâš™ï¸ LLM ì„¤ì • ë³€ê²½")
    print("=" * 40)
    
    config = current_config.copy()
    
    # ëª¨ë¸ ì„ íƒ
    print(f"í˜„ì¬ ëª¨ë¸: {config['model']}")
    new_model = input("ìƒˆ ëª¨ë¸ëª… (Enterë¡œ ìœ ì§€): ").strip()
    if new_model:
        config['model'] = new_model
    
    # Temperature
    print(f"í˜„ì¬ temperature: {config['temperature']}")
    try:
        new_temp = input("ìƒˆ temperature (0.0-1.0, Enterë¡œ ìœ ì§€): ").strip()
        if new_temp:
            temp_val = float(new_temp)
            if 0.0 <= temp_val <= 1.0:
                config['temperature'] = temp_val
            else:
                print("âŒ temperatureëŠ” 0.0-1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # num_predict
    print(f"í˜„ì¬ num_predict: {config['num_predict']}")
    try:
        new_predict = input("ìƒˆ num_predict (í† í° ìˆ˜, Enterë¡œ ìœ ì§€): ").strip()
        if new_predict:
            predict_val = int(new_predict)
            if predict_val > 0:
                config['num_predict'] = predict_val
            else:
                print("âŒ num_predictëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # top_k
    print(f"í˜„ì¬ top_k: {config['top_k']}")
    try:
        new_top_k = input("ìƒˆ top_k (Enterë¡œ ìœ ì§€): ").strip()
        if new_top_k:
            top_k_val = int(new_top_k)
            if top_k_val > 0:
                config['top_k'] = top_k_val
            else:
                print("âŒ top_këŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # top_p
    print(f"í˜„ì¬ top_p: {config['top_p']}")
    try:
        new_top_p = input("ìƒˆ top_p (0.0-1.0, Enterë¡œ ìœ ì§€): ").strip()
        if new_top_p:
            top_p_val = float(new_top_p)
            if 0.0 <= top_p_val <= 1.0:
                config['top_p'] = top_p_val
            else:
                print("âŒ top_pëŠ” 0.0-1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    # timeout
    print(f"í˜„ì¬ timeout: {config['timeout']}ì´ˆ")
    try:
        new_timeout = input("ìƒˆ timeout (ì´ˆ, Enterë¡œ ìœ ì§€): ").strip()
        if new_timeout:
            timeout_val = int(new_timeout)
            if timeout_val > 0:
                config['timeout'] = timeout_val
            else:
                print("âŒ timeoutì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    except ValueError:
        print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    print(f"\nâœ… ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    return config

# ==========================
# âœ… 7. ì‹¤í–‰ í•¨ìˆ˜
# ==========================
def main(question: str, embedding_index, top_k: int = 5, llm_config: Optional[dict] = None, streaming: bool = False, use_parallel: bool = True, max_workers: int = 4, split_report: bool = True):
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    split_report: Trueë©´ ê° í•­ëª©ë³„ë¡œ ë¶„í•  ìƒì„± ë° ì¶œë ¥, Falseë©´ ê¸°ì¡´ ì „ì²´ ìƒì„±
    """
    mode = determine_mode(question)
    print(f"\nğŸ“Œ ê°ì§€ëœ ëª¨ë“œ: {mode}\n")

    if mode == "free":
        print("ğŸ’¬ ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹ â†’ LLM ì§ì ‘ ë‹µë³€\n")
        
        # ë¬¸ì„œ ê²€ìƒ‰ ì‹œë„ (ì„ íƒì )
        relevant_docs = ""
        if embedding_index is not None:
            relevant_docs = search_relevant_documents(question, embedding_index, k=2, max_chars=8000)
        
        try:
            # LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            free_llm_config = DEFAULT_LLM_CONFIG.copy()
            if llm_config is not None:
                free_llm_config.update(llm_config)
            free_llm_config["num_predict"] = 2000  # ììœ ì§ˆë¬¸ìš©ìœ¼ë¡œ í† í° ìˆ˜ ì¡°ì •
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì°¸ê³ , ì—†ìœ¼ë©´ ì¼ë°˜ ë‹µë³€)
            if relevant_docs:
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ:
{relevant_docs}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            else:
                prompt = f"""ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€:"""
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                headers={"Content-Type": "application/json"},
                data=json.dumps({
                    "model": free_llm_config["model"],
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": free_llm_config["temperature"],
                        "num_predict": free_llm_config["num_predict"],
                        "top_k": free_llm_config["top_k"],
                        "top_p": free_llm_config["top_p"]
                    }
                }),
                timeout=free_llm_config["timeout"]
            )
            if response.status_code == 200:
                resp_json = response.json()
                if "response" in resp_json:
                    print(resp_json["response"])
                else:
                    print("âŒ LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
            else:
                print("âŒ LLM API ì˜¤ë¥˜")
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return

    # ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸° (ë¬¸ì„œ í¬ê¸° ì œí•œ ì ìš©)
    if embedding_index is None:
        print("âŒ ì„ë² ë”© ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
        
    source_text, sources = search_relevant_documents(question, embedding_index, k=top_k, max_chars=15000)
    search_success = bool(source_text)
    
    if not source_text:
        print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if mode == "report" and split_report:
        print("\nğŸª„ ë³´ê³ ì„œ ê° í•­ëª©ì„ ë¶„í• ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n")
        fields = [
            ("abstract", "ìš”ì•½"),
            ("introduction", "ì„œë¡ "),
            ("background", "ë°°ê²½"),
            ("main_content", "ë³¸ë¬¸"),
            ("conclusion", "ê²°ë¡ ")
        ]
        parsed = {}
        for field, label in fields:
            print(f"\nâ³ [{label}] ìƒì„± ì¤‘...")
            val = regenerate_field(field, source_text, question, llm_config)
            parsed[field] = val if val else "ë‚´ìš© ì—†ìŒ"
            print(f"\nâœ… [{label}]\n{val if val else 'ë‚´ìš© ì—†ìŒ'}\n")
        # ì œëª©ì€ ìš”ì•½ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜, ë³„ë„ ìƒì„±(ê°„ë‹¨í™”)
        parsed["title"] = question.strip()[:40] + ("..." if len(question.strip()) > 40 else "")
        from utils.output_format import format_report_output
        print("\nğŸ“ ì „ì²´ ë³´ê³ ì„œ(ëª¨ì•„ë³´ê¸°):\n")
        print(format_report_output(parsed, mode, sources))
        # í”¼ë“œë°± ë° í›„ì† ë£¨í”„ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
        def save_feedback_log(question: str, answer: str, sources: list, feedback: str):
            import datetime
            with open('feedback_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"=== {datetime.datetime.now()} ===\n")
                f.write(f"ì§ˆë¬¸: {question}\n")
                f.write(f"ì¶œì²˜: {', '.join(sources)}\n")
                f.write(f"í”¼ë“œë°±: {feedback}\n")
                f.write(f"ë‹µë³€:\n{answer}\n\n")
        try:
            feedback = input("ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”? (y/n): ").strip().lower()
            if feedback in ('y', 'n'):
                from utils.output_format import format_report_output
                answer_text = format_report_output(parsed, mode, sources)
                save_feedback_log(question, answer_text, sources, feedback)
                print("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
            followup = input("ë³´ê³ ì„œì— ëŒ€í•´ ì¶”ê°€ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°ì‹œ ê±´ë„ˆëœ€): ").strip()
            if followup:
                modification_keywords = ["ìˆ˜ì •", "ë°”ê¿”", "ê³ ì³", "ë‹¤ì‹œ", "ì¬ì‘ì„±", "ë³€ê²½", "ì¡°ì •", "ê°œì„ "]
                is_modification_request = any(keyword in followup for keyword in modification_keywords)
                if is_modification_request and mode == "report" and parsed:
                    print("ğŸ”„ ë³´ê³ ì„œ ìˆ˜ì • ìš”ì²­ ê°ì§€ - JSON í˜•ì‹ìœ¼ë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                    modification_prompt = f"""
ê¸°ì¡´ ë³´ê³ ì„œë¥¼ ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì— ë”°ë¼ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ê¸°ì¡´ ë³´ê³ ì„œ]
{answer_text}

[ì‚¬ìš©ì ìˆ˜ì • ìš”ì²­]
{followup}

ìš”êµ¬ì‚¬í•­:
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (JSON ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)
- ë‹¤ìŒ í•„ë“œë§Œ í¬í•¨: title, abstract, introduction, background, main_content, conclusion
- sections ë°°ì—´ì´ë‚˜ ë‹¤ë¥¸ í•„ë“œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
- ê° í•„ë“œëŠ” 300ì ì´ìƒ
- ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì„ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ê°œì„ í•˜ì„¸ìš”

âš ï¸ ì—„ê²©í•œ ê·œì¹™:
- JSON ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ ë¬¸ì„œì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”
- ëª¨ë“  ë‚´ìš©ì€ ìì‹ ë§Œì˜ ì–¸ì–´ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
"""
                    from utils.llm import LLMClient
                    llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
                    modified_response = llm.generate(modification_prompt, num_predict=3000, temperature=DEFAULT_LLM_CONFIG["temperature"])
                    modified_parsed = llm.parse_response(modified_response)
                    if isinstance(modified_parsed, dict) and not modified_parsed.get("error"):
                        print("\n===== ìˆ˜ì •ëœ ë³´ê³ ì„œ =====")
                        from utils.output_format import format_report_output
                        print(format_report_output(modified_parsed, mode, sources))
                        parsed.update(modified_parsed)
                    else:
                        print("\n===== ìˆ˜ì • ìš”ì²­ ë‹µë³€ =====")
                        print(modified_response)
                else:
                    from utils.llm import LLMClient
                    llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
                    followup_prompt = f"""
ì•„ë˜ëŠ” ê¸°ì¡´ ë³´ê³ ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¶”ê°€ ìš”ì²­(ìˆ˜ì •/ì§ˆë¬¸ ë“±)ì— ë”°ë¼ ì ì ˆíˆ ë‹µë³€í•˜ê±°ë‚˜, í•´ë‹¹ ë¶€ë¶„ì„ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.

[ê¸°ì¡´ ë³´ê³ ì„œ]
{answer_text}

[ì‚¬ìš©ì ì¶”ê°€ ìš”ì²­]
{followup}

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""
                    followup_answer = llm.generate(followup_prompt, num_predict=1500, temperature=DEFAULT_LLM_CONFIG["temperature"])
                    print("\n===== ì¶”ê°€ ìš”ì²­/ìˆ˜ì •/ë‹µë³€ ê²°ê³¼ =====\n")
                    print(followup_answer)
        except Exception as e:
            print(f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return

    # ê¸°ì¡´ ì „ì²´ ìƒì„± ë°©ì‹ (split_report=False)
    # ê¸°ë³¸ ìƒíƒœ ìƒì„±
    state_dict = {
        "source_text": source_text,
        "question": question,
        "top_k": top_k,
        "prompt": "",
        "response": "",
        "parsed": {},
        "error": "",
        "retry_count": 0,
        "mode": mode,
        "search_results": [],
        "search_success": search_success,
        "streaming_enabled": streaming
    }
    
    # LLM ì„¤ì •ì´ ì œê³µëœ ê²½ìš° ìƒíƒœì— ì¶”ê°€
    if llm_config:
        state_dict["llm_config"] = llm_config
    
    initial_state = ReportState(**state_dict)
    final_state = graph.invoke(initial_state)

    # ìµœì¢… ìƒíƒœì—ì„œ ê²°ê³¼ ì¶”ì¶œ (LangGraph ê²°ê³¼ëŠ” ì§ì ‘ final_state)
    source_text = final_state.get("source_text", "")
    parsed = final_state.get("parsed", {})
    error = final_state.get("error", "")
    # íŒŒì‹± ê²°ê³¼ê°€ dictê°€ ì•„ë‹ˆë©´ dictë¡œ ê°ì‹¸ê¸°
    if not isinstance(parsed, dict):
        parsed = {"raw_response": str(parsed)}
    
    # ê²°ê³¼ ì¶œë ¥
    if parsed and not final_state["error"]:
        # ë³´ê³ ì„œ ëª¨ë“œì¼ ë•Œ ê° í•„ë“œ ê¸¸ì´ ê²€ì‚¬ ë° ìë™ ë³´ì™„
        if mode == "report":
            min_lengths = {
                "abstract": 300,
                "introduction": 300,
                "background": 300,
                "main_content": 300,
                "conclusion": 300
            }
            updated = False
            for field, min_len in min_lengths.items():
                val = parsed.get(field, "")
                if not val or len(str(val).strip()) < min_len:
                    print(f"[ìë™ ë³´ì™„] {field} í•­ëª©ì´ {min_len}ì ë¯¸ë§Œì…ë‹ˆë‹¤. LLMìœ¼ë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                    new_val = regenerate_field(field, source_text, question, llm_config)
                    if new_val and len(str(new_val).strip()) >= min_len:
                        parsed[field] = new_val
                        updated = True
            if updated:
                print("[ìë™ ë³´ì™„] ì¼ë¶€ í•­ëª©ì´ ë³´ì™„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        from utils.output_format import format_report_output
        print(format_report_output(parsed, mode, sources))
    elif parsed and "raw_response" in parsed:
        # íŒŒì‹± ì‹¤íŒ¨í–ˆì§€ë§Œ ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
        raw_response = remove_duplicates(parsed["raw_response"])
        print(f"""
{'='*60}
âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ - ì›ë³¸ ì‘ë‹µ ì¶œë ¥
{'='*60}

{raw_response}

{'='*60}""")
    elif error:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {error}")
        # ì˜¤ë¥˜ê°€ ìˆì§€ë§Œ ì›ë³¸ ì‘ë‹µì´ ìˆëŠ” ê²½ìš° ì¶œë ¥
        if "response" in final_state and final_state["response"]:
            raw_response = remove_duplicates(final_state["response"])
            print(f"""
{'='*60}
ğŸ“ LLM ì›ë³¸ ì‘ë‹µ
{'='*60}

{raw_response}

{'='*60}""")
        if source_text:
            print(f"""
ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
{'-' * 30}
{source_text[:500]}{'...' if len(source_text) > 500 else ''}""")
    else:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ")
        if source_text:
            print(f"""
ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
{'-' * 30}
{source_text[:500]}{'...' if len(source_text) > 500 else ''}""")

    # ê²°ê³¼ ì¶œë ¥ ì´í›„ì— í”¼ë“œë°± ì €ì¥ í•¨ìˆ˜ ì •ì˜ ë° í˜¸ì¶œ
    def save_feedback_log(question: str, answer: str, sources: list, feedback: str):
        import datetime
        with open('feedback_log.txt', 'a', encoding='utf-8') as f:
            f.write(f"=== {datetime.datetime.now()} ===\n")
            f.write(f"ì§ˆë¬¸: {question}\n")
            f.write(f"ì¶œì²˜: {', '.join(sources)}\n")
            f.write(f"í”¼ë“œë°±: {feedback}\n")
            f.write(f"ë‹µë³€:\n{answer}\n\n")
    # í”¼ë“œë°± ë£¨í”„
    try:
        feedback = input("ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”? (y/n): ").strip().lower()
        if feedback in ('y', 'n'):
            # ë‹µë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            answer_text = ''
            if parsed and not final_state["error"]:
                from utils.output_format import format_report_output
                answer_text = format_report_output(parsed, mode, sources)
            elif parsed and "raw_response" in parsed:
                answer_text = parsed["raw_response"]
            elif final_state.get("response"):
                answer_text = final_state["response"]
            else:
                answer_text = str(parsed)
            save_feedback_log(question, answer_text, sources, feedback)
            print("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
        # ì¶”ê°€ í”¼ë“œë°±/ìˆ˜ì •/ì§ˆë¬¸ ë£¨í”„
        followup = input("ë³´ê³ ì„œì— ëŒ€í•´ ì¶”ê°€ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°ì‹œ ê±´ë„ˆëœ€): ").strip()
        if followup:
            # ìˆ˜ì • ìš”ì²­ì¸ì§€ í™•ì¸ (íŠ¹ì • í‚¤ì›Œë“œë¡œ íŒë‹¨)
            modification_keywords = ["ìˆ˜ì •", "ë°”ê¿”", "ê³ ì³", "ë‹¤ì‹œ", "ì¬ì‘ì„±", "ë³€ê²½", "ì¡°ì •", "ê°œì„ "]
            is_modification_request = any(keyword in followup for keyword in modification_keywords)
            
            if is_modification_request and mode == "report" and parsed and not final_state["error"]:
                # JSON í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ì¬ìƒì„±
                print("ğŸ”„ ë³´ê³ ì„œ ìˆ˜ì • ìš”ì²­ ê°ì§€ - JSON í˜•ì‹ìœ¼ë¡œ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                modification_prompt = f"""
ê¸°ì¡´ ë³´ê³ ì„œë¥¼ ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì— ë”°ë¼ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ê¸°ì¡´ ë³´ê³ ì„œ]
{answer_text}

[ì‚¬ìš©ì ìˆ˜ì • ìš”ì²­]
{followup}

ìš”êµ¬ì‚¬í•­:
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥ (JSON ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)
- ë‹¤ìŒ í•„ë“œë§Œ í¬í•¨: title, abstract, introduction, background, main_content, conclusion
- ëª¨ë“  í•„ë“œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±
- ê° í•„ë“œëŠ” 300ì ì´ìƒ
- ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì„ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ê°œì„ í•˜ì„¸ìš”

âš ï¸ ì—„ê²©í•œ ê·œì¹™:
- JSON ì™¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì›ë³¸ ë¬¸ì„œì˜ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”
- ëª¨ë“  ë‚´ìš©ì€ ìì‹ ë§Œì˜ ì–¸ì–´ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”
"""
                
                try:
                    from utils.llm import LLMClient
                    llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
                    modified_response = llm.generate(modification_prompt, num_predict=3000, temperature=DEFAULT_LLM_CONFIG["temperature"])
                    
                    # ìˆ˜ì •ëœ ì‘ë‹µ íŒŒì‹±
                    modified_parsed = llm.parse_response(modified_response)
                    
                    if isinstance(modified_parsed, dict) and not modified_parsed.get("error"):
                        print("\n===== ìˆ˜ì •ëœ ë³´ê³ ì„œ =====")
                        from utils.output_format import format_report_output
                        print(format_report_output(modified_parsed, mode, sources))
                        
                        # ìˆ˜ì •ëœ ê²°ê³¼ë¥¼ ì›ë³¸ì— ë°˜ì˜
                        parsed.update(modified_parsed)
                    else:
                        print("\n===== ìˆ˜ì • ìš”ì²­ ë‹µë³€ =====")
                        print(modified_response)
                        
                except Exception as e:
                    print(f"ë³´ê³ ì„œ ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì¼ë°˜ ë‹µë³€ìœ¼ë¡œ fallback
                    from utils.llm import LLMClient
                    llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
                    followup_prompt = f"""
ì•„ë˜ëŠ” ê¸°ì¡´ ë³´ê³ ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¶”ê°€ ìš”ì²­(ìˆ˜ì •/ì§ˆë¬¸ ë“±)ì— ë”°ë¼ ì ì ˆíˆ ë‹µë³€í•˜ê±°ë‚˜, í•´ë‹¹ ë¶€ë¶„ì„ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.

[ê¸°ì¡´ ë³´ê³ ì„œ]
{answer_text}

[ì‚¬ìš©ì ì¶”ê°€ ìš”ì²­]
{followup}

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""
                    followup_answer = llm.generate(followup_prompt, num_predict=1500, temperature=DEFAULT_LLM_CONFIG["temperature"])
                    print("\n===== ì¶”ê°€ ìš”ì²­/ìˆ˜ì •/ë‹µë³€ ê²°ê³¼ =====\n")
                    print(followup_answer)
            else:
                # ì¼ë°˜ ì§ˆë¬¸/ë‹µë³€
                from utils.llm import LLMClient
                llm = LLMClient(model_name=DEFAULT_LLM_CONFIG["model"])
                followup_prompt = f"""
ì•„ë˜ëŠ” ê¸°ì¡´ ë³´ê³ ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¶”ê°€ ìš”ì²­(ìˆ˜ì •/ì§ˆë¬¸ ë“±)ì— ë”°ë¼ ì ì ˆíˆ ë‹µë³€í•˜ê±°ë‚˜, í•´ë‹¹ ë¶€ë¶„ì„ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.

[ê¸°ì¡´ ë³´ê³ ì„œ]
{answer_text}

[ì‚¬ìš©ì ì¶”ê°€ ìš”ì²­]
{followup}

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""
                followup_answer = llm.generate(followup_prompt, num_predict=1500, temperature=DEFAULT_LLM_CONFIG["temperature"])
                print("\n===== ì¶”ê°€ ìš”ì²­/ìˆ˜ì •/ë‹µë³€ ê²°ê³¼ =====\n")
                print(followup_answer)
    except Exception as e:
        print(f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

# ==========================
# âœ… 8. CLI ì¸í„°í˜ì´ìŠ¤
# ==========================
def run_cli():
    """CLI ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë° ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
    print("=" * 60)
    print("ğŸ“š RAG LangGraph QA ì‹œìŠ¤í…œ (ê°œì„ ëœ ë²„ì „)")
    print("ğŸ¤– Ollama Mistral ëª¨ë¸ ê¸°ë°˜")
    print("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì§€ì›")
    print("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›")
    print("ğŸ“Š ì§„í–‰ë¥  í‘œì‹œ ì§€ì›")
    print("ğŸ“ ë¡œê¹… ì‹œìŠ¤í…œ í†µí•©")
    print("=" * 60)
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (ì„¤ì • íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    use_parallel = config_manager.is_parallel_enabled()
    max_workers = config_manager.get_max_workers()
    
    # ì„ë² ë”© ì¸ë±ìŠ¤ ìë™ ìƒì„±/ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    logger.info("ì„ë² ë”© ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘...")
    embedding_index = get_or_build_embedding_index(use_parallel=use_parallel, max_workers=max_workers)
    if embedding_index is None:
        logger.error("ì„ë² ë”© ì¸ë±ìŠ¤ ì¤€ë¹„ ì‹¤íŒ¨")
        print("âŒ ì„ë² ë”© ì¸ë±ìŠ¤ ì¤€ë¹„ ì‹¤íŒ¨. ë¬¸ì„œë¥¼ ./docs/embedding/ì— ë„£ì–´ì£¼ì„¸ìš”!")
        return
    logger.info("ì„ë² ë”© ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
    
    # í˜„ì¬ LLM ì„¤ì •
    current_config = DEFAULT_LLM_CONFIG.copy()
    streaming_mode = False
    
    print("\nğŸ“– ì‚¬ìš©ë²•:")
    print("  â€¢ ìš”ì•½: 'ìš”ì•½í•´ì¤˜', 'ê°„ë‹¨íˆ' ë“±ì˜ í‚¤ì›Œë“œ")
    print("  â€¢ ë³´ê³ ì„œ: 'ë³´ê³ ì„œë¡œ ì‘ì„±í•´ì¤˜', 'êµ¬ì¡°í™”í•´ì„œ' ë“±ì˜ í‚¤ì›Œë“œ")
    print("  â€¢ ì¼ë°˜ì§ˆë¬¸: ê·¸ ì™¸ ëª¨ë“  ì§ˆë¬¸ (ë¬¸ì„œ ì°¸ê³  + ì¼ë°˜ ì§€ì‹)")
    print("  â€¢ ìŠ¤íŠ¸ë¦¬ë°: 'stream' ì…ë ¥ í›„ ì§ˆë¬¸")
    print("  â€¢ ë³‘ë ¬ì²˜ë¦¬: 'parallel' ì…ë ¥ í›„ on/off")
    print("  â€¢ ì›Œì»¤ìˆ˜: 'workers' ì…ë ¥ í›„ ìˆ«ì")
    print("  â€¢ ì„¤ì • ë³€ê²½: 'config' ì…ë ¥")
    print("  â€¢ ì¢…ë£Œ: 'quit', 'exit', 'q'")
    
    print(f"\nâš™ï¸ í˜„ì¬ ì„¤ì •:")
    print(f"  â€¢ ë³‘ë ¬ ì²˜ë¦¬: {'í™œì„±í™”' if use_parallel else 'ë¹„í™œì„±í™”'}")
    print(f"  â€¢ ì›Œì»¤ ìˆ˜: {max_workers}")
    print(f"  â€¢ ìŠ¤íŠ¸ë¦¬ë°: {'í™œì„±í™”' if streaming_mode else 'ë¹„í™œì„±í™”'}")
    
    while True:
        try:
            question = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            
            if not question:
                continue
                
            if question.lower() in ['quit', 'exit', 'q']:
                logger.info("ì‚¬ìš©ìê°€ ì‹œìŠ¤í…œì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
                print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            if question.lower() == 'config':
                current_config = configure_llm_settings(current_config)
                continue
                
            if question.lower() == 'stream':
                streaming_mode = not streaming_mode
                status = "í™œì„±í™”" if streaming_mode else "ë¹„í™œì„±í™”"
                print(f"ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ {status}")
                print(f"âš™ï¸ í˜„ì¬ ì„¤ì •: ë³‘ë ¬ ì²˜ë¦¬ {'í™œì„±í™”' if use_parallel else 'ë¹„í™œì„±í™”'}, ì›Œì»¤ ìˆ˜ {max_workers}, ìŠ¤íŠ¸ë¦¬ë° {status}")
                continue
                
            if question.lower() == 'parallel':
                use_parallel = not use_parallel
                status = "í™œì„±í™”" if use_parallel else "ë¹„í™œì„±í™”"
                print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ {status}")
                print(f"âš™ï¸ í˜„ì¬ ì„¤ì •: ë³‘ë ¬ ì²˜ë¦¬ {status}, ì›Œì»¤ ìˆ˜ {max_workers}")
                continue
                
            if question.lower() == 'workers':
                try:
                    new_workers = int(input("ì›Œì»¤ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-8): "))
                    if 1 <= new_workers <= 8:
                        max_workers = new_workers
                        status = "í™œì„±í™”" if use_parallel else "ë¹„í™œì„±í™”"
                        print(f"âš¡ ì›Œì»¤ ìˆ˜ê°€ {max_workers}ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        print(f"âš™ï¸ í˜„ì¬ ì„¤ì •: ë³‘ë ¬ ì²˜ë¦¬ {status}, ì›Œì»¤ ìˆ˜ {max_workers}")
                    else:
                        print("âŒ ì›Œì»¤ ìˆ˜ëŠ” 1-8 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
                except ValueError:
                    print("âŒ ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                continue
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë‹¤ìŒ ì§ˆë¬¸ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬
            main(question, embedding_index, llm_config=current_config, streaming=streaming_mode, use_parallel=use_parallel, max_workers=max_workers)
            
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìê°€ Ctrl+Cë¡œ ì‹œìŠ¤í…œì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
            print("\n\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            logger.error(f"CLI ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# âœ… ì§„ì…ì 
if __name__ == "__main__":
    run_cli() 